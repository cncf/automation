defaultRules:
  create: true
  rules:
    alertmanager: false
    etcd: false
    configReloaders: false
    general: false
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: false
    kubeApiserverBurnrate: false
    kubeApiserverHistogram: false
    kubeApiserverSlos: false
    kubeControllerManager: false
    kubelet: true
    kubeProxy: false
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
    windows: false

kubeStateMetrics:
  enabled: true

kubeApiServer:
  enabled: true

kubeControllerManager:
  enabled: false

kubeEtcd:
  enabled: false

kubeScheduler:
  enabled: false

kubeProxy:
  enabled: true

coreDns:
  enabled: true

kubeDns:
  enabled: false

grafana:
  admin:
    existingSecret: "grafana-credentials"
    userKey: admin-user
    passwordKey: admin-password

alertmanager:
  enabled: true
  alertmanagerSpec:
    secrets:
      - alertmanager-secrets
  config:
    global:
      resolve_timeout: 5m
      slack_api_url_file: '/etc/alertmanager/secrets/alertmanager-secrets/slack-api-url'
    route:
      group_by: ['resource','service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'slack'
      routes:
      - receiver: 'null'
        matchers:
          - alertname =~ "InfoInhibitor|Watchdog"
    receivers:
    - name: 'null'
    - name: 'slack'
      slack_configs:
      - channel: '#internal-gha-prmths-alrtmngr'
        send_resolved: true
        title: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
        text: "{{ range .Alerts }}<!channel> {{ .Annotations.description }}\n{{ end }}"
        footer: ":kubernetes: *oci-gha-amd64-runners*"
    templates:
    - '/etc/alertmanager/config/*.tmpl'

prometheus:
  additionalPodMonitors:
  - name: "gha-rs-controller"
    podMetricsEndpoints:
    - interval: 15s
      path: /metrics
      scheme: http
      portNumber: 8080
    namespaceSelector:
      matchNames:
      - arc-systems
    selector:
      matchLabels:
        app.kubernetes.io/name: gha-rs-controller
  - name: "gha-rs-listener"
    podMetricsEndpoints:
    - interval: 15s
      path: /metrics
      scheme: http
      portNumber: 8080
    namespaceSelector:
      matchNames:
      - arc-systems
    selector:
      matchLabels:
        app.kubernetes.io/component: runner-scale-set-listener

additionalPrometheusRulesMap:
  rule-name:
    groups:
      - name: gha.rules
        rules:
          - alert: GHA Listener at High Capacity
            expr: sum(label_replace(gha_desired_runners, "runner", "$1", "pod", "(oracle-.+)-([a-z0-9]+)-listener$")) by (runner) / sum(label_replace(gha_max_runners, "runner", "$1", "pod", "(oracle-.+)-([a-z0-9]+)-listener$")) by (runner) * 100 >= 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: GHA Listener is at high capacity
              description: "{{ $labels.runner }}) is at {{ $value }}% capacity"

          - alert: GHA Listener at Max Capacity
            expr: sum(label_replace(gha_desired_runners, "runner", "$1", "pod", "(oracle-.+)-([a-z0-9]+)-listener$")) by (runner) / sum(label_replace(gha_max_runners, "runner", "$1", "pod", "(oracle-.+)-([a-z0-9]+)-listener$")) by (runner) * 100 >= 99
            for: 1m
            labels:
              severity: major
            annotations:
              summary: GHA Listener is at max capacity
              description: "{{ $labels.runner }}) is at {{ $value }}% capacity"

          - alert: GHA Pending EphemeralRunners
            expr: sum(gha_controller_pending_ephemeral_runners) by (name) > 10
            for: 1m
            labels:
              severity: warning
            annotations:
              summary: GHA Pending EphemeralRunners
              description: "There are {{ $value }} pending EphemeralRunners for {{ $labels.name }}"

          - alert: GHA Failed EphemeralRunners
            expr: sum(gha_controller_failed_ephemeral_runners) by (name) > 5
            for: 1m
            labels:
              severity: warning
            annotations:
              summary: GHA Failed EphemeralRunners
              description: "There are {{ $value }} failed EphemeralRunners for {{ $labels.name }}"
